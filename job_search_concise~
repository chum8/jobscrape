# import os
# this file simply compiles data concerning where the most jobs per city are
import csv, requests
filename1, filename2 = 'results-largest-cities-01.csv', 'results-summed-largest-cities-01.csv'
city_col, state_col = 1, 2
split_word="+"
states = []
with open(filename1) as f1:
    reader = csv.reader(f1)
    with open(filename2, "w+") as f2:
        writer = csv.writer(f2, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
        reading_first = True
        for row in reader:
            current_row = []
            if reading_first:
                last_state_value = row[0]
                last_count_value = row[21]
                reading_first = False
            if row[0] == last_state_value
                last_count_value = row[21]
            else:
                current_row.append(last_state_value)
                current_row.append(last_count_value)
                writer.writerow(current_row)

            last_state_value = row[0]
            last_count_value = row[21]


        header_row.append(row[0])
        # build search string
        temp = search_q + split_word.join( x for x in row[0].split(" ")) + search_l
        # print(temp) # debug line
        my_searches.append(temp)
        my_searches.sort()
    header_row.append("City Total")
    header_row.append("State Total")
    
with open(filename3, "w+") as f3:
    writer = csv.writer(f3, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
    writer.writerow(header_row)

    with open(filename2) as f2:
        reader = csv.reader(f2)
        my_locs = []
        for row in reader:
            temp = row[state_col] + ", " + row[city_col]
            my_locs.append(temp)
            my_locs.sort()

    cur_loc = 0
    loc_count = len(my_locs)
    old_state = my_locs[0].split(", ")[0] # initialize the first state
    state_count = 0
    for loc in my_locs:
        cur_loc += 1
        print("Scraping",str(cur_loc),"of",str(loc_count) + ": " + loc + "...")
        current_row = loc.split(", ")
        current_state = loc.split(", ")[0]
        city_count = 0
        for search in my_searches:
            jobs = 0
            try:
                url = str(search + loc)
                page = requests.get(url)
                tree = html.fromstring(page.content)
                # lazy way to see if there are no jobs
                try:
                    #jobs = tree.xpath("//description")
                    jobs = int(tree.xpath("//div[@id='searchCount']/text()")[0].split()[3])
                    # jobs = tree.xpath("//meta[@name='Description']")
                    #jobs = int(tree.xpath(//div[@id="searchCount"]/text()).split(" ")[3])
                except:
                    # do nothing
                    i = 0
        
                # print(url+ ": " + str(jobs)) # debug

            except:
                print("Something went wrong.  Perhaps the page is no longer accepting requests.")
                # add a little pause
                for i in range(0, 100000):
                    i = i
            city_count += jobs
            current_row.append(jobs)

        # if we are still counting results from the same state
        if current_state == old_state:
            state_count += city_count
        # otherwise initialize a new state and write state count to current row
        else:
            state_count = city_count
            old_state = current_state
        current_row.append(city_count)
        current_row.append(state_count)
        writer.writerow(current_row)
